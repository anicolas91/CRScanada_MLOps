{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRS Canada immigration metrics analysis\n",
    "We investigate the Comprehensive Ranking System (CRS) metrics that canada uses to grant permanent residence invitations.\n",
    "**Goal: to project predicted cutoff values for the incoming cohort.**\n",
    "\n",
    "We essentially:\n",
    "1. Scrub the website and create a pandas dataframe\n",
    "2. Clean up the data\n",
    "3. Create the preprocessing variables\n",
    "4. Fit a model\n",
    "5. Evaluate the goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general libraries\n",
    "import os\n",
    "import pickle\n",
    "import mlflow\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML models\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_website(url):\n",
    "    ''' \n",
    "    This function scrapes data from the website and converts it to a usable pandas framework.\n",
    "    '''\n",
    "    # read in json filetype\n",
    "    r = requests.get(url)\n",
    "    rounds = r.json()['rounds']\n",
    "    # remove commas from integer strings\n",
    "    for r in rounds:\n",
    "        for i in r:\n",
    "            r[i] = r[i].replace(\",\", \"\")\n",
    "    # create pandas df from dictionary\n",
    "    df = pd.DataFrame.from_dict(rounds,dtype='string')\n",
    "    # specify column names as needed\n",
    "    columns={\"drawNumber\": \"id\", \n",
    "             \"drawDate\": \"date\", \n",
    "             \"drawName\": \"round type\", \n",
    "             \"drawSize\":\"invitations issued\", \n",
    "             \"drawCRS\": \"CRS cutoff\",\n",
    "             \"drawText2\": \"type issued\",\n",
    "             \"drawCutOff\": \"tie break rule\",\n",
    "             \"dd18\": \"total applications\",\n",
    "             \"dd1\":  \"crs_range_601_1200\",\n",
    "             \"dd2\":  \"crs_range_501_600\",\n",
    "             \"dd3\":  \"crs_range_451_500\",\n",
    "             \"dd4\":  \"crs_range_491_500\",\n",
    "             \"dd5\":  \"crs_range_481_490\",\n",
    "             \"dd6\":  \"crs_range_471_480\",\n",
    "             \"dd7\":  \"crs_range_461_470\",\n",
    "             \"dd8\":  \"crs_range_451_460\",\n",
    "             \"dd9\":  \"crs_range_401_450\",\n",
    "             \"dd10\": \"crs_range_441_450\",\n",
    "             \"dd11\": \"crs_range_431_440\",\n",
    "             \"dd12\": \"crs_range_421_430\",\n",
    "             \"dd13\": \"crs_range_411_420\",\n",
    "             \"dd14\": \"crs_range_401_410\",\n",
    "             \"dd15\": \"crs_range_351_400\",\n",
    "             \"dd16\": \"crs_range_301_350\",\n",
    "             \"dd17\": \"crs_range_000_300\",\n",
    "             }\n",
    "    # specify dtypes as needed\n",
    "    dtypes={#\"drawNumber\": \"id\", \n",
    "            \"drawDate\": \"datetime64[ns]\", \n",
    "            #\"drawName\": \"round type\", \n",
    "            \"drawSize\":\"int64\", \n",
    "            \"drawCRS\": \"int64\",\n",
    "            #\"drawText2\": \"type issued\",\n",
    "            #\"drawCutOff\": \"tie break rule\",\n",
    "            \"dd18\": \"int64\",\n",
    "            \"dd1\":  \"int64\",\n",
    "            \"dd2\":  \"int64\",\n",
    "            \"dd3\":  \"int64\",\n",
    "            \"dd4\":  \"int64\",\n",
    "            \"dd5\":  \"int64\",\n",
    "            \"dd6\":  \"int64\",\n",
    "            \"dd7\":  \"int64\",\n",
    "            \"dd8\":  \"int64\",\n",
    "            \"dd9\":  \"int64\",\n",
    "            \"dd10\": \"int64\",\n",
    "            \"dd11\": \"int64\",\n",
    "            \"dd12\": \"int64\",\n",
    "            \"dd13\": \"int64\",\n",
    "            \"dd14\": \"int64\",\n",
    "            \"dd15\": \"int64\",\n",
    "            \"dd16\": \"int64\",\n",
    "            \"dd17\": \"int64\",\n",
    "            }\n",
    "    #set names and dtypes\n",
    "    df = df.astype(dtype=dtypes)\n",
    "    df = df.rename(columns=columns)\n",
    "    # extract only columns of interest\n",
    "    vars_to_keep=['date','round type','invitations issued','CRS cutoff']\n",
    "    df= df[vars_to_keep]\n",
    "    #setup date itself as the index\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    return df\n",
    "\n",
    "def cleanup_df_general_rounds(df):\n",
    "    ''' \n",
    "    this function extracts only the general rounds and cleans up data slightly\n",
    "    '''\n",
    "    # combine general rounds into one type\n",
    "    df[\"round type\"] = df[\"round type\"].replace({'No Program Specified': 'General'})\n",
    "    # extract gral rounds only and remove outliers\n",
    "    df = df[(df[\"round type\"] == \"General\") & ((df[\"CRS cutoff\"] < 700) & (df[\"CRS cutoff\"] > 100))]\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_date_vars(df):\n",
    "    ''' \n",
    "    this function calculates extra variables based on date\n",
    "    '''\n",
    "    # get date column\n",
    "    dates = df.index.to_series()\n",
    "    # get vars\n",
    "    df['month'] = dates.dt.month\n",
    "    df['year'] = dates.dt.year\n",
    "    df['dayofweek'] = dates.dt.dayofweek\n",
    "    df['quarter'] = dates.dt.quarter\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_rolling_averages(df,roll_times=['30D','60D','90D','180D']):\n",
    "    ''' \n",
    "    This function calculates the rolling averages point wise for the uneven datetimes\n",
    "    '''\n",
    "    # flip data from oldest to newest\n",
    "    df = df.iloc[::-1].copy()\n",
    "    # get mean CRS in the past N months prior to this value\n",
    "    for r in roll_times:\n",
    "        df['roll_'+r] = df['CRS cutoff'].rolling(r, min_periods=1,closed='left').mean()\n",
    "    # flip from new to old\n",
    "    df = df.iloc[::-1]\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_offset_windows(df,offset_value=[-1,-2,-3]):\n",
    "    # remove nans so that you dont use their datetimes\n",
    "    df = df.dropna().copy()\n",
    "    # get date column\n",
    "    dates = df.index.to_series()\n",
    "    # get window per each offset values\n",
    "    for offset in offset_value:\n",
    "        #add previous CRS\n",
    "        df['CRS'+str(offset)] = df['CRS cutoff'].shift(offset)\n",
    "        #calculate quickly dt\n",
    "        df['dt'+str(offset)] = dates.diff(periods=offset).abs().dt.days#.shift(offset)\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def calculate_independent_vars(df):\n",
    "    ''' \n",
    "    this function calculates all independent variables in one go.\n",
    "    '''\n",
    "    # calculate rolling averages\n",
    "    df = calculate_rolling_averages(df)\n",
    "    # calculate offset windows\n",
    "    df = calculate_offset_windows(df)\n",
    "    # calculate data variables\n",
    "    df = calculate_date_vars(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_from_url(url):\n",
    "    ''' \n",
    "    This function scrubs the url and returns a cleaned up dataframe with all independent vars\n",
    "    '''\n",
    "    df = create_df_from_website(url)\n",
    "    df = cleanup_df_general_rounds(df)\n",
    "    df = calculate_independent_vars(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_test_train(df,split_date='01-Jan-2023'):\n",
    "    ''' \n",
    "    this function splits the time series into test and train data based on a cutoff date\n",
    "    '''\n",
    "    # get date column\n",
    "    dates = df.index.to_series()\n",
    "    # split it\n",
    "    data_train = df.loc[dates <= split_date].copy()\n",
    "    data_test = df.loc[dates > split_date].copy()\n",
    "    return data_train, data_test\n",
    "\n",
    "def create_features(data,x_labels=['dt-1','CRS-1'],y_labels=['CRS cutoff']):\n",
    "    ''' \n",
    "    This function creates X and Y features\n",
    "    '''\n",
    "    # split in to X and Y\n",
    "    X = data[x_labels]\n",
    "    Y = data[y_labels]\n",
    "    return X, Y\n",
    "\n",
    "def create_split_features(df,split_date='01-Jan-2023',x_labels=['dt-1','CRS-1'],y_labels=['CRS cutoff']):\n",
    "    ''' \n",
    "    This function performs in one sitting the splitting into test,train, and independent, dependent vars\n",
    "    '''\n",
    "    # split data according to date\n",
    "    dtrain, dtest = split_test_train(df,split_date)\n",
    "    # extract the labels\n",
    "    X_train, y_train = dtrain[x_labels], dtrain[y_labels]\n",
    "    X_test, y_test = dtest[x_labels], dtest[y_labels]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def fit_and_evaluate_model(X_train,y_train,X_test,y_test,type='linear'):\n",
    "    ''' \n",
    "    this function fits either an XGboost or a linear model and returns model/rmse\n",
    "    '''\n",
    "    # select model type\n",
    "    if type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif type == 'xgboost':\n",
    "        model = get_best_xgboost_model(X_train, y_train, X_test, y_test)\n",
    "    #fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # calculate the RMSE\n",
    "    y_pred = y_test.copy()\n",
    "    y_pred['CRS cutoff'] = model.predict(X_test)\n",
    "    rmse = root_mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    # return fitted model and rmse\n",
    "    return model, rmse, y_pred\n",
    "\n",
    "def get_best_xgboost_model(X_train, y_train, X_test, y_test):\n",
    "    ''' \n",
    "    this function is an auxiliary as it gets the best parameters for xgboost\n",
    "    '''\n",
    "    #set up parameter range to search\n",
    "    parameters = {\n",
    "        'n_estimators': [100,250,500],\n",
    "        'learning_rate': [0.01,0.05,0.1, 0.5, 0.9],\n",
    "        'max_depth': [10,15,20],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "    # set up eval set\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    #search the grid\n",
    "    model = xgb.XGBRegressor(eval_set=eval_set, objective='reg:squarederror', verbose=False)\n",
    "    clf = GridSearchCV(model, parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # establish the best parameters as the precursors for the model\n",
    "    print(f'Best params: {clf.best_params_}')\n",
    "    print(f'Best validation score = {clf.best_score_}')\n",
    "    #output the model\n",
    "    model = xgb.XGBRegressor(**clf.best_params_, objective='reg:squarederror')\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_model_to_pickle(model,filename='../models/lin_reg.bin'):\n",
    "    ''' \n",
    "    this aux function saves the fitted model to a bin pickle file\n",
    "    '''\n",
    "    # create any directories if does not exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    #save file\n",
    "    with open(filename, 'wb') as f_out:\n",
    "        pickle.dump(model, f_out)\n",
    "\n",
    "    return None\n",
    "\n",
    "def plot_and_save_results(y_train,y_test,y_pred,filename='./figures/demo-file.png'):\n",
    "    ''' \n",
    "    this aux function creates a pandas df plot and saves it as a png\n",
    "    ''' \n",
    "    # calculates rmse\n",
    "    rmse = root_mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    # create any directories if does not exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    # plot the figure via pandas\n",
    "    ax = y_train.rename(columns={'CRS cutoff': 'Train Data'}).plot(figsize=(15, 5),title = f'rmse: {rmse:.3f}',ylabel='CRS cutoff',style='.-')\n",
    "    y_test.rename(columns={'CRS cutoff': 'Test Data'}).plot(ax=ax,style='.-')\n",
    "    y_pred.rename(columns={'CRS cutoff': 'Prediction from model'}).plot(ax=ax,style='.-',grid=True)\n",
    "    #save the figure\n",
    "    ax.figure.savefig(filename)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup\n",
    "\n",
    "#### MLflow\n",
    "1. Need to run on terminal: `mlflow ui --backend-store-uri sqlite:///mlflow.db`\n",
    "2. MLflow can be found on [http://127.0.0.1:5000/](http://127.0.0.1:5000/)\n",
    "3. We set the tracking uri on the python script\n",
    "4. We set the experiment name where all runs will be saved. It the exp doesn't exist mlflow will create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow setup\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"CRS-score-canada\")\n",
    "\n",
    "# preprocessing\n",
    "url = 'https://www.canada.ca/content/dam/ircc/documents/json/ee_rounds_123_en.json'\n",
    "df = preprocess_from_url(url)\n",
    "\n",
    "# create features and such\n",
    "split_date = '01-Jan-2023'\n",
    "y_labels=['CRS cutoff']\n",
    "\n",
    "# establish the combination of independent vars we wish to study\n",
    "xlabels_combos = [\n",
    "    ['roll_30D','roll_60D','roll_90D','dt-1','CRS-1'],\n",
    "    ['roll_30D','roll_60D','roll_90D','roll_180D','dt-1','CRS-1'],\n",
    "    ['roll_30D','roll_60D','roll_90D','dt-1','CRS-1','month','year'],\n",
    "    ['roll_30D','roll_60D','roll_90D','dt-1','CRS-1','dt-2','CRS-2','dt-3','CRS-3'],\n",
    "    ['roll_30D','roll_60D','roll_90D','dt-1','CRS-1','dt-2','CRS-2','dt-3','CRS-3','month','year'],\n",
    "    ['roll_30D','roll_60D','dt-1','CRS-1','month','year','quarter'],\n",
    "    ['dt-1','CRS-1','dt-2','CRS-2','dt-3','CRS-3'],\n",
    "    ['dt-1','CRS-1','dt-2','CRS-2','dt-3','CRS-3','month','year'], \n",
    "    ['roll_30D','roll_60D','roll_90D','roll_180D','dt-1','CRS-1','dt-2','CRS-2','dt-3','CRS-3','month','year','quarter'],\n",
    "    ['month','year','quarter','dayofweek'] \n",
    "]\n",
    "\n",
    "#specify model types\n",
    "model_types = ['linear','xgboost']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start runs at each model type and xlabel combo\n",
    "for model_type in model_types:\n",
    "    for x_labels in xlabels_combos:\n",
    "        # start a run\n",
    "        with mlflow.start_run():\n",
    "            # obtain features\n",
    "            X_train, y_train, X_test, y_test = create_split_features(\n",
    "                                                df=df,\n",
    "                                                split_date=split_date,\n",
    "                                                x_labels=x_labels,\n",
    "                                                y_labels=y_labels\n",
    "                                                )\n",
    "            # model fitting\n",
    "            model, rmse, y_pred = fit_and_evaluate_model(X_train,y_train,X_test,y_test,type=model_type)\n",
    "            # save model as bin file\n",
    "            model_filename = f'./models/{model_type}_model.bin'\n",
    "            save_model_to_pickle(model,filename=model_filename)\n",
    "            # plot fig and save as png\n",
    "            figure_filename = f'./figures/{model_type}_model.png'\n",
    "            ax = plot_and_save_results(y_train,y_test,y_pred,filename=figure_filename)\n",
    "            #register everything to mlflow\n",
    "            #note that there are other ways to register, but for our needs we are keeping it simple\n",
    "            mlflow.set_tag(\"model_type\",model_type)\n",
    "            mlflow.set_tag(\"developer\",\"andrea\")\n",
    "            mlflow.log_param(\"x_labels\",x_labels)\n",
    "            mlflow.log_param(\"split_date\",split_date)\n",
    "            mlflow.log_metric(\"rmse\",rmse)\n",
    "            mlflow.log_artifact(local_path=model_filename, artifact_path=\"models\")\n",
    "            mlflow.log_artifact(local_path=figure_filename, artifact_path=\"figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish\n",
    "This notebook can now be exported as a python script via: \n",
    "```bash\n",
    "jupyter nbconvert --to script main.ipynb\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
